# ğŸ” JSON Duplicate Finder

This Python script detects duplicate records across multiple JSON files based on one or more user-defined keys. It's designed to be configurable, timestamped, and provide both machine-readable and human-readable reports.

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ config.json
â”œâ”€â”€ find_duplicates.py
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ file1.json
â”‚   â”œâ”€â”€ file2.json
â”‚   â””â”€â”€ ...
```

---

## âš™ï¸ Configuration

Edit the `config.json` file to define which files to process and which fields to use for identifying duplicates.

### Example:
```json
{
  "json_files": [
    "data/file1.json",
    "data/file2.json",
    "data/file3.json"
  ],
  "duplicate_keys": ["itemCode", "storeNum"]
}
```

- `json_files`: List of JSON files to be scanned (must be array of records).
- `duplicate_keys`: One or more keys that together define a unique row.

---

## ğŸš€ How to Run

Make sure you're using Python 3.7+.

### Step 1: Install dependencies (none required for base version)

```bash
pip install -r requirements.txt  # Optional - no third-party libraries needed
```

### Step 2: Run the script

```bash
python find_duplicates.py
```

---

## ğŸ“¦ Output

Each run generates:

- `duplicates_<timestamp>.json` â€” full duplicate data
- `duplicates_<timestamp>.txt` â€” readable summary including:
  - File-by-file record count
  - Total records
  - Duplicate key summary
  - Actual duplicates

---

## âœ… Sample Console Output

```
ğŸ“Š Summary
Total files checked: 3
  - data/file1.json: 120 record(s)
  - data/file2.json: 95 record(s)
  - data/file3.json: 140 record(s)
Combined total records: 355
Found 4 duplicate key(s), 9 total duplicate rows.
ğŸ“ JSON saved to: duplicates_2025-05-09_1640.json
ğŸ“ TXT summary saved to: duplicates_2025-05-09_1640.txt
```

---

## ğŸ§ª Sample Data Format

Each file should contain a **JSON array** like:

```json
[
  { "itemCode": "A100", "storeNum": "S001", "qty": 5 },
  { "itemCode": "A101", "storeNum": "S002", "qty": 3 }
]
```

---

## ğŸ“Œ Notes

- Fields with `null` or `missing` values for any `duplicate_keys` are **ignored** in duplication checks.
- You can run this script multiple times â€” results are saved with timestamped filenames.
- Handles JSON errors gracefully and skips unreadable or malformed files.

---

## ğŸ› ï¸ Future Enhancements (Optional)

- Group duplicates by source file
- Support for nested JSON paths
- Interactive CLI or web UI

---

## ğŸ‘¨â€ğŸ’» Author

Sumit Mathur  
ğŸ“§ srmathur@hotmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/sumit-mathur-3aa98b9/)
